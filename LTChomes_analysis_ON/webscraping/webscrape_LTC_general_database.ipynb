{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last scraped June 10, 2020\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltc_requests = requests.get(\"http://publicreporting.ltchomes.net/en-ca/Search_Selection.aspx\")\n",
    "ltc = BeautifulSoup(ltc_requests.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ltc_requests.status_code\n",
    "# ltc_requests.reason\n",
    "# print(ltc_requests.request.headers)\n",
    "# ltc_requests.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrape list of LTC home names and links to detailed info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webscrape list of ltc home names and links\n",
    "ordered_lists = ltc.find_all(\"ol\")\n",
    "ltc_list = ordered_lists[1]\n",
    "home = ltc_list.find_all(\"a\", {\"class\":\"rsLink\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of names and links\n",
    "names = []\n",
    "links = []\n",
    "for each in home:\n",
    "    names.append(each.getText())\n",
    "    links.append(each.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651 651 651\n",
      "651 651\n"
     ]
    }
   ],
   "source": [
    "# there are 651 records which is correct\n",
    "print(len(home), len(names), len(links))\n",
    "\n",
    "# we see that the names are all unique\n",
    "print(len(set(names)), len(set(links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify links list to full url\n",
    "full_links = []\n",
    "for i in range(len(links)):\n",
    "    full_links.append('http://publicreporting.ltchomes.net/en-ca/' + links[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscrape detailed profile info for each LTC home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each of the links and scrape characteristics of ltc homes\n",
    "addresses = []\n",
    "cities_postalcodes = []\n",
    "LHIN = []\n",
    "licensee = []\n",
    "management = []\n",
    "home_type = []\n",
    "beds = []\n",
    "short_stay = []\n",
    "residents_council = []\n",
    "family_council = []\n",
    "accreditation = []\n",
    "info = []\n",
    "counter = -1\n",
    "for each in full_links:\n",
    "    counter += 1\n",
    "#     print(counter)\n",
    "    soup = requests.get(each)\n",
    "    soup = BeautifulSoup(soup.text)\n",
    "    addresses.append(soup.find(\"div\", {\"id\":\"ctl00_ContentPlaceHolder1_divHomeAddress\"}).getText())\n",
    "    cities_postalcodes.append(soup.find(\"div\", {\"id\":\"ctl00_ContentPlaceHolder1_divHomeCity\"}).getText())\n",
    "    profiles = soup.find(\"div\", {\"id\":\"ctl00_ContentPlaceHolder1_divHomeProfile_item_Col1\"})\n",
    "    profile_data = profiles.find_all(\"div\", {\"class\":\"Profilerow_col2\"})\n",
    "    try:    \n",
    "        LHIN.append(profile_data[0].getText())\n",
    "        licensee.append(profile_data[3].getText())\n",
    "        management.append(profile_data[4].getText())\n",
    "        home_type.append(profile_data[5].getText())\n",
    "        beds.append(profile_data[6].getText())\n",
    "        short_stay.append(profile_data[7].getText())\n",
    "        residents_council.append(profile_data[8].getText())\n",
    "        family_council.append(profile_data[9].getText())\n",
    "        accreditation.append(profile_data[10].getText())\n",
    "        info.append(profile_data[12].getText())\n",
    "     \n",
    "    except:\n",
    "        print(counter) # print out records which resulted in error\n",
    "        LHIN.append(None)\n",
    "        licensee.append(None)\n",
    "        management.append(None)\n",
    "        home_type.append(None)\n",
    "        beds.append(None)\n",
    "        short_stay.append(None)\n",
    "        residents_council.append(None)\n",
    "        family_council.append(None)\n",
    "        accreditation.append(None)\n",
    "        info.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two homes do not have profile info, tag for removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out links to LTC homes for which an error was raised during webscraping\n",
    "print(full_links[324]) # LENNOX AND ADDINGTON COUNTY GENERAL HOSPITAL\n",
    "print(full_links[508]) # ST. JOSEPH'S MOTHER HOUSE (MARTHA WING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df of Ontario LTC homes\n",
    "df = pd.DataFrame({'name': names, \n",
    "                   'address':addresses, \n",
    "                   'city_and_postal_code':cities_postalcodes, \n",
    "                   'LHIN':LHIN, \n",
    "                   'licensee':licensee, \n",
    "                   'management':management, \n",
    "                   'home_type':home_type, \n",
    "                   'beds': beds, \n",
    "                   'short_stay':short_stay, \n",
    "                   'residents_council':residents_council, \n",
    "                   'family_council':family_council, \n",
    "                   'accreditation':accreditation, \n",
    "                   'additional_info':info})\n",
    "df.info()\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the 'city and postal code' column into 2 columns 'city' and 'postal code'\n",
    "df['city'] = df['city_and_postal_code'].str.split(',').str[0]\n",
    "df['postal_code'] = df['city_and_postal_code'].str.split(',').str[1]\n",
    "\n",
    "# use regex to extract the number of beds to a different column\n",
    "df['number_of_beds'] = df['beds'].str.extract(r'(\\d+)', expand=False)\n",
    "\n",
    "df.head(20)\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out LTC homes with duplicate addresses\n",
    "df[df.duplicated(['address'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual review of 3 duplicated addresses involving 6 homes and tag for removal as needed\n",
    "- Cedarwood: no website\n",
    "- Great Northern: home closed in 2013\n",
    "- Lakeland Eldcap: no website, seems to be connected to Lakeland LTC\n",
    "- Lakeland LTC: http://www.lakelandltc.com/, owned and connected to WPSHC which is a hospital\n",
    "- Harmony: https://www.siennaliving.ca/long-term-care/ontario/harmony-hills-care-community, last inspection Feb 2020\n",
    "- Fountain: https://www.siennaliving.ca/long-term-care/ontario/fountain-view-care-community, last inspection Oct 2019\n",
    "\n",
    "\n",
    "## Webscrape inspections info for each LTC home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify links to access inspection data \n",
    "full_links_inspection = [each + '&tab=1' for each in full_links]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webscrape inspection data for each ltc home \n",
    "frames = []\n",
    "counter = -1\n",
    "for each in full_links_inspection:\n",
    "    inspection_types = []\n",
    "    inspection_dates = []\n",
    "#     counter += 1\n",
    "#     print(counter)\n",
    "    soup = requests.get(each)\n",
    "    soup = BeautifulSoup(soup.text)\n",
    "    name = soup.find(\"div\", {\"class\":\"HomeName\"}).getText()\n",
    "    inspections = soup.find(\"div\", {\"id\":\"ctl00_ContentPlaceHolder1_divHomeProfile_item_Col3\"})\n",
    "    types = inspections.find_all(\"div\", {\"class\":\"divInspectionTypeDataCol\"})\n",
    "    number_of_types = len(types)\n",
    "    for each in types:\n",
    "        inspection_types.append(each.getText())\n",
    "    dates = inspections.find_all(\"div\", {\"class\":\"divInspectionDateDataCol\"})\n",
    "    number_of_dates = len(dates)\n",
    "    for each in dates:\n",
    "        inspection_dates.append(each.getText())\n",
    "    df_temp = pd.DataFrame({\"name\": name, \n",
    "                            \"inspection_types\":inspection_types, \n",
    "                            \"number_of_types\":number_of_types, \n",
    "                            \"inspection_dates\":inspection_dates, \n",
    "                            \"number_of_dates\":number_of_dates})\n",
    "    frames.append(df_temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a df of raw inspection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df of inspection data\n",
    "df2 = pd.concat(frames)\n",
    "print(df2.nunique()) # there are only 648 unique LTC home names\n",
    "df2.info()\n",
    "\n",
    "# display data for first 2 ltc homes\n",
    "pd.set_option('display.max_rows', None)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date column into date datatype\n",
    "df2['inspection_dates'] = pd.to_datetime(df2['inspection_dates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list 29 different inspection types\n",
    "df2['inspection_types'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return LTC home names that do not match with original list of names\n",
    "orig = set(df['name'])\n",
    "inspec = set(df2['name'])\n",
    "def returnNotMatches(a, b):\n",
    "    return [[x for x in a if x not in b], [x for x in b if x not in a]]\n",
    "returnNotMatches(orig, inspec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual review of 3 LTC homes that were not represented in the inspections data and tag for removal\n",
    "- ST. JOSEPH'S MOTHER HOUSE (MARTHA WING), no inspections or profile information\n",
    "- NORTHUMBERLAND HILLS HOSPITAL: No inspections, closed 2012\n",
    "- MARIANHILL - MARGUERITE CENTRE: No inspections, closed 2012\n",
    "\n",
    "\n",
    "## Enumerate number of inspections for each LTC home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each home identify date of first inspection\n",
    "min_dates = df2.groupby(['name'])['inspection_dates'].min()\n",
    "df2['first_inspection_date'] = df2.apply(lambda row: min_dates.loc[row['name']], axis=1)\n",
    "df2.head()\n",
    "\n",
    "# create a df with total number of inspections\n",
    "df_total = df2.groupby(['name']).size().to_frame('total_inspections').reset_index()\n",
    "df_total.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with date of first inspection\n",
    "df_first = df2[['name', 'first_inspection_date']]\n",
    "df_first.drop_duplicates(inplace=True)\n",
    "df_first.info()\n",
    "df_first.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot a histogram of total inspections to see how the number of inspections are distributed\n",
    "# df_total.hist(column='total_inspections')\n",
    "\n",
    "# # cut total inspections into quartiles and add a column with quartile ranges\n",
    "# df_total['quartiles_total_range'] = pd.qcut(df_total['total_inspections'], q=4, precision=0)\n",
    "# print(df_total['quartiles_total_range'].value_counts())\n",
    "\n",
    "# # add column with quartile rank values\n",
    "# df_total['quartiles_total_rank'] = pd.qcut(df_total['total_inspections'], q=4, labels = False, precision=0)\n",
    "\n",
    "# df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df2 and keep only rows with inspections since Jan 2015 inclusive (inspections in the last 5 years)\n",
    "df_5y = df2[(df2['inspection_dates'] > '2015-01-01')]\n",
    "\n",
    "# create df with number of inspections in the last 5 years\n",
    "df_5y = df_5y.groupby(['name']).size().to_frame('5y_inspections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot a histogram of 5y inspections to see how the number of inspections are distributed\n",
    "# df_5y.hist(column='5y_inspections')\n",
    "\n",
    "# # cut total inspections into quartiles and add a column with quartile ranges\n",
    "# df_5y['quartiles_5y_range'] = pd.qcut(df_5y['5y_inspections'], q=4, precision=0)\n",
    "# print(df_5y['quartiles_5y_range'].value_counts())\n",
    "\n",
    "# # add column with quartile rank values\n",
    "# df_5y['quartiles_5y_rank'] = pd.qcut(df_5y['5y_inspections'], q=4, labels = False, precision=0)\n",
    "\n",
    "df_5y.info()\n",
    "# df_5y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df2 and keep only rows with inspections since Jan 2018 inclusive (inspections in the last 2 years)\n",
    "df_2y = df2[(df2['inspection_dates'] > '2018-01-01')]\n",
    "\n",
    "# create df with number of inspections in the last 2 years\n",
    "df_2y = df_2y.groupby(['name']).size().to_frame('2y_inspections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot a histogram of 2y inspections to see how the number of inspections are distributed\n",
    "# df_2y.hist(column='2y_inspections')\n",
    "\n",
    "# # cut total inspections into quartiles and add a column with quartile ranges\n",
    "# df_2y['quartiles_2y_range'] = pd.qcut(df_2y['2y_inspections'], q=4, precision=0)\n",
    "# print(df_2y['quartiles_2y_range'].value_counts())\n",
    "\n",
    "# # add column with quartile rank values\n",
    "# df_2y['quartiles_2y_rank'] = pd.qcut(df_2y['2y_inspections'], q=4, labels = False, precision=0)\n",
    "\n",
    "df_2y.info()\n",
    "# df_2y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that there are fewer homes with inspections in the last 5y/2y meaning that some homes have not been inspected in the last 2-5 years. Some homes may be closed. These have not yet been filtered out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerate the number of inspections containing the words \"Compliants\", \"Critical Incident\" and \"Order(s)\" for each home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complaints = df2[df2[\"inspection_types\"].str.contains('Complaints', regex=False, na=False) ]\n",
    "df_complaints_total = df_complaints.groupby(['name']).size().to_frame('total_complaints').reset_index()\n",
    "\n",
    "df_critical = df2[df2[\"inspection_types\"].str.contains('Critical Incident', regex=False, na=False) ]\n",
    "df_critical_total = df_critical.groupby(['name']).size().to_frame('total_critical').reset_index()\n",
    "\n",
    "df_withOrders = df2[df2[\"inspection_types\"].str.contains('Order(s)', regex=False, na=False) ]\n",
    "df_withOrders_total = df_withOrders.groupby(['name']).size().to_frame('total_withOrders').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complaints, critical and withOrders in the last 5y\n",
    "df_complaints = df2[df2[\"inspection_types\"].str.contains('Complaints', regex=False, na=False) ]\n",
    "df_complaints_5y = df_complaints[(df_complaints['inspection_dates'] > '2015-01-01')]\n",
    "df_complaints_5y = df_complaints_5y.groupby(['name']).size().to_frame('5y_complaints').reset_index()\n",
    "\n",
    "df_critical = df2[df2[\"inspection_types\"].str.contains('Critical Incident', regex=False, na=False) ]\n",
    "df_critical_5y = df_critical[(df_critical['inspection_dates'] > '2015-01-01')]\n",
    "df_critical_5y = df_critical_5y.groupby(['name']).size().to_frame('5y_critical').reset_index()\n",
    "\n",
    "df_withOrders = df2[df2[\"inspection_types\"].str.contains('Order(s)', regex=False, na=False) ]\n",
    "df_withOrders_5y = df_withOrders[(df_withOrders['inspection_dates'] > '2015-01-01')]\n",
    "df_withOrders_5y = df_withOrders_5y.groupby(['name']).size().to_frame('5y_withOrders').reset_index()\n",
    "\n",
    "# Complaints, critical and withOrders in the last 2y\n",
    "df_complaints = df2[df2[\"inspection_types\"].str.contains('Complaints', regex=False, na=False) ]\n",
    "df_complaints_2y = df_complaints[(df_complaints['inspection_dates'] > '2018-01-01')]\n",
    "df_complaints_2y = df_complaints_2y.groupby(['name']).size().to_frame('2y_complaints').reset_index()\n",
    "\n",
    "df_critical = df2[df2[\"inspection_types\"].str.contains('Critical Incident', regex=False, na=False) ]\n",
    "df_critical_2y = df_critical[(df_critical['inspection_dates'] > '2018-01-01')]\n",
    "df_critical_2y = df_critical_2y.groupby(['name']).size().to_frame('2y_critical').reset_index()\n",
    "\n",
    "df_withOrders = df2[df2[\"inspection_types\"].str.contains('Order(s)', regex=False, na=False) ]\n",
    "df_withOrders_2y = df_withOrders[(df_withOrders['inspection_dates'] > '2018-01-01')]\n",
    "df_withOrders_2y = df_withOrders_2y.groupby(['name']).size().to_frame('2y_withOrders').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect the multiple dataframes\n",
    "# df_total.info()\n",
    "# df_5y = df_5y.reset_index(); df_5y.info()\n",
    "# df_2y = df_2y.reset_index(); df_2y.info()\n",
    "# df_complaints_total.info() # 9243 complaints\n",
    "# df_critical_total.info() # 7905 critical \n",
    "# df_withOrders_total.info() # 4305 orders\n",
    "# df_complaints_5y.info() \n",
    "# df_critical_5y.info()\n",
    "# df_withOrders_5y.info()\n",
    "# df_complaints_2y.info()\n",
    "# df_critical_2y.info()\n",
    "# df_withOrders_2y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Merge the dataframes on home name\n",
    "from functools import reduce\n",
    "list_inspections_dfs = [df_first, df_total, df_5y, df_2y, \n",
    "                        df_complaints_total, df_complaints_5y, df_complaints_2y, \n",
    "                        df_critical_total, df_critical_5y, df_critical_2y,\n",
    "                        df_withOrders_total, df_withOrders_5y, df_withOrders_2y]\n",
    "\n",
    "df_inspections = reduce(lambda x,y: pd.merge(x,y, on='name', how='outer'), list_inspections_dfs)\n",
    "df_inspections.info()\n",
    "df_inspections.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the inspections data with the profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dfs\n",
    "ltc_scrape = pd.merge(left=df, right=df_inspections, how='left', left_on='name', right_on='name')\n",
    "\n",
    "# Replace blank values with NaN\n",
    "ltc_scrape = ltc_scrape.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "ltc_scrape.info()\n",
    "ltc_scrape.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove inactive homes\n",
    "- Homes with 'closed' in additional info\n",
    "- Homes missing all profile information\n",
    "- Homes with no inspection reports in the last 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that 20 homes are closed, and one home was merged\n",
    "ltc_scrape['additional_info'] = ltc_scrape['additional_info'].str.lower()\n",
    "# ltc_scrape['additional_info'].value_counts()\n",
    "ltc_scrape.additional_info.str.contains(\"closed\", na=False).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with homes that are closed\n",
    "ltc_scrape = ltc_scrape[~ltc_scrape.additional_info.str.contains(\"closed\", na=False)].reset_index()\n",
    "\n",
    "# Review other additional info, there are 5 homes with additional info\n",
    "ltc_scrape.loc[ltc_scrape['additional_info'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Review of one home that was merged in Jan 2016\n",
    "\n",
    "- Address to the merged home is just a general PO BOX to the licensee\n",
    "- Will remove the merged home from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rows with information on the 2 homes that were merged\n",
    "ltc_scrape[ltc_scrape['name'].str.contains('MOUNT HOPE', regex=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltc_scrape.iloc[487]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop row with the merged home\n",
    "ltc_scrape = ltc_scrape.drop(ltc_scrape.index[487])\n",
    "\n",
    "# Check to see it is gone\n",
    "ltc_scrape[ltc_scrape['name'].str.contains('MOUNT HOPE', regex=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review homes with missing profile information\n",
    "ltc_scrape.loc[ltc_scrape['LHIN'].isna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following homes are missing all profile information:\n",
    "- LENNOX AND ADDINGTON COUNTY GENERAL HOSPITAL\n",
    "    - lennox and addington county general hospital: missing all profile information (eg. LHIN, accreditation etc.), https://www.southeasthealthline.ca/displayService.aspx?id=151718, 312, 22 beds convalescent (90 days) and resident long term care at a hospital, last inspection 2019, publically funded, LHIN South East, not in quality database\n",
    "- ST. JOSEPH'S MOTHER HOUSE (MARTHA WING)\n",
    "    - no inspections data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing profile information\n",
    "ltc_scrape = ltc_scrape[ltc_scrape.name != 'LENNOX AND ADDINGTON COUNTY GENERAL HOSPITAL']\n",
    "ltc_scrape = ltc_scrape[ltc_scrape.name != 'ST. JOSEPH\\'S MOTHER HOUSE (MARTHA WING)']\n",
    "ltc_scrape.loc[ltc_scrape['LHIN'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review homes without any inspections since January 2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltc_scrape.loc[ltc_scrape['2y_inspections'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following homes have no inspections data since Jan 2018:\n",
    "1. LADY ISABELLE NURSING HOME \n",
    "    - http://www.ladyisabelle.ca/Contact_Information.html, MISSING from ODHF, missing 2y_inspections, according to web search this home was closed by the ministry in 2014, in april 2020 there are plans to reopen/redevelop a LTC home here, will remove from this analysis\n",
    "2. MALDEN PARK CONTINUING CARE CENTRE\n",
    "    - seems to be closed, last inspection 2010, telephone disconnected\n",
    "3. PEOPLE CARE CENTRE Stratford\n",
    "    - listed in ODHF but no geodata, https://www.peoplecare.ca/, closed after flood in 2015 per websearch, tel number does not work, missing 5y_inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with no inspections since 2018\n",
    "ltc_scrape.dropna(subset = ['2y_inspections'], inplace = True)\n",
    "ltc_scrape.loc[ltc_scrape['2y_inspections'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export final dataframe with LTC profile and inspections info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltc_scrape.info()\n",
    "ltc_scrape.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export final df as csv\n",
    "ltc_scrape.to_csv(r'webscrape_LTC_general_database.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test scripts by scraping a single ltc site\n",
    "\n",
    "# scrape profile data\n",
    "# soup = requests.get(\n",
    "# \"http://publicreporting.ltchomes.net/en-ca/homeprofile.aspx?Home=2872\") \n",
    "# soup = BeautifulSoup(soup.text)\n",
    "\n",
    "# address = soup.find(\"div\", {\"id\":\"ctl00_ContentPlaceHolder1_divHomeAddress\"}).getText()\n",
    "# city = soup.find(\"div\", {\"id\":\"ctl00_ContentPlaceHolder1_divHomeCity\"}).getText()\n",
    "# profiles = soup.find(\"div\", {\"id\":\"ctl00_ContentPlaceHolder1_divHomeProfile_item_Col1\"})\n",
    "# profile_data = profiles.find_all(\"div\", {\"class\":\"Profilerow_col2\"})\n",
    "# profile_data\n",
    "# LHIN = profile_data[0].getText()\n",
    "# LHIN\n",
    "\n",
    "# # scrape inspections data\n",
    "# soup = requests.get(\n",
    "# \"http://publicreporting.ltchomes.net/en-ca/homeprofile.aspx?Home=2872&tab=1\") # add \"&tab=1\" to url\n",
    "# soup = BeautifulSoup(soup.text)\n",
    "\n",
    "# inspections = soup.find(\"div\", {\"id\":\"ctl00_ContentPlaceHolder1_divHomeProfile_item_Col3\"})\n",
    "# inspection_types = inspections.find_all(\"div\", {\"class\":\"divInspectionTypeDataCol\"})\n",
    "# inspection_dates = inspections.find_all(\"div\", {\"class\":\"divInspectionDateDataCol\"})\n",
    "\n",
    "# print(len(inspection_types))\n",
    "# print(len(inspection_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = []\n",
    "# for each in inspection_types:\n",
    "#     a.append(each.getText())\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.to_csv(r'ltc_covid_odhf_qual_FOR_STATS.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
