---
title: "ltc_analysis_ON_share"
output: html_document
---

## Import and prepare the data
```{r }
# library(skimr)
library(devtools)
library(broom)

# Import the data
df <- read.csv("../data_output/merged_LTC_odhf_quality.csv", na.strings="", header=TRUE)
# skim(df)
# head(df)
# str(df)

# Change variable data types
# To factor
df[, 'outbreak'] <- as.factor(df[, 'outbreak'])

# To datetime
cols <- c( "first_inspection_date", 
           "last_inspection_date", 
           "last_rqi_date",
           "date_first_outbreak", 
           "date_max_cases", 
           "date_max_HCWcases", 
           "date_first_resolved")
df[cols] <- lapply(df[cols], as.Date)

# To char
cols <- c("cleaned_name", 
          "name", 
          "address", 
          "city", 
          "postal_code", 
          "CSDname")
df[cols] <- lapply(df[cols], as.character)


```

```{r }
# # Add a column of LHIN health regions
# unique(df[['LHIN']])
# df$region[df$LHIN=='North West' | df$LHIN=='North East'] <- "North"
# df$region[df$LHIN=='Toronto Central'] <- "Toronto"
# df$region[df$LHIN=='Champlain' | df$LHIN=='Central East' | df$LHIN=='South East'] <- "East"
# df$region[df$LHIN=='Erie St. Clair' | df$LHIN=='Hamilton Niagara Haldimand Brant (Hnhb)' | df$LHIN=='South West' | df$LHIN=='Waterloo Wellington'] <- "West"
# df$region[df$LHIN=='Mississauga Halton' | df$LHIN=='Central West' | df$LHIN=='Central' | df$LHIN=='North Simcoe Muskoka'] <- "Central"

```


## Determine number years since the first inspection for each home
```{r, results='hide'}
# Create a new column for first inspection year
df$first_inspection_year <- format(df$first_inspection_date,"%Y")
df$first_inspection_year <- as.numeric(df$first_inspection_year)

# Earliest date of first inspection was in 2009 and latest date was 2016
# summary(df[['first_inspection_year']])

# Create a new column with number of years since the first inspection was reported
df$years_since_first <- 2020 - df$first_inspection_year
# summary(df[['years_since_first']])

# There is only 1 home with fewer than 5 years of inspections -> Royal Rose Place 293
table(df['years_since_first'])
subset(df, years_since_first == 4)
```

### Normalize the number of inspections
```{r, results='hide'}

# Number of inspections/beds
df$total_inspections_norm <- df$total_inspections/df$years_since_first
df$X5y_inspections_norm <- df$X5y_inspections/5
df[293, "X5y_inspections_norm"] <- df[293, "X5y_inspections_norm"]*5/4
df$X2y_inspections_norm <- df$X2y_inspections/2

# Number of withOrders/beds
df$total_withOrders_norm <- df$total_withOrders/df$years_since_first
df$X5y_withOrders_norm <- df$X5y_withOrders/5
df[293, "X5y_withOrders_norm"] <- df[293, "X5y_withOrders_norm"]*5/4
df$X2y_withOrders_norm <- df$X2y_withOrders/2

# Drop un-normalized inspections data
# df <- df[,-c(21:32)]

dput(names(df))
# skim(df)
```


```{r }
cor(df$number_beds, df$X2y_inspections_norm)
```

```{r, }
# Generate the data for analysis
# Create lists of variables used to filter data for analysis

outcomes = c(
  "outbreak", 
  "cum_resident_deaths", 
  "max_resident_cases", 
  "max_hcw_cases"
  )

profile = c(
  "home_type", 
  "number_beds",
  "short_stay",
  "residents_council",
  "family_council",
  "accreditation"
  )

qual = c(
  "antipsychotic_percent", 
  "pressure_ulcers_percent", 
  "falls_percent", 
  "restraints_percent", 
  "depression_percent", 
  "pain_percent"
  )

location = c(
  'address', 
  'LHIN', 
  'city', 
  'postal_code', 
  'CSDname', 
  'CSDuid', 
  'latitude', 
  'longitude')

inspections_total = c(
  'total_inspections_norm',
  'total_withOrders_norm'
)

inspections_5y = c(
    'X5y_inspections_norm'
  , "X5y_withOrders_norm"
  )

inspections_2y = c(
    'X2y_inspections_norm'
  , "X2y_withOrders_norm"
  )

inspections = c(inspections_2y, inspections_5y, inspections_total)

```

## Analysis with all 625 homes (exclude Quality data)
```{r }
data <- subset(df, select = c('outbreak', profile, inspections))
# skim(data)
# View(colnames(data))
# dput(names(df))

```

## LASSO CovTest
```{r }
options(max.print=999999)
library(covTest)
x = model.matrix(outbreak~., data = data)[,-1]
y = as.numeric(data.matrix(data[,1]))

a = lars.glm(x,y,family="binomial")
a$beta
a$act
a$pathobj
a$a0

covTest(a,x,y)



```

## Fit logistic regression
```{r }
fit = glm(outbreak~., family='binomial', data=data)
summary(fit)

```

## Check multicollinearity
```{r }
car::vif(fit)
```

## LASSO for variable selection - manual
```{r }
# https://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome
library(glmnet)

# model.matrix() automatically transforms any qualitative variables into dummy variables
# Trim the first column leaving only predictors
# x = model.matrix(outbreak~., data = data)[,-1]
# y = as.factor(data.matrix(data[,1]))

# Note alpha=1 for lasso only, and can blend with ridge penalty
# alpha=0 ridge only
glmmod <- glmnet(x, y, family = 'binomial')

# glmmod$call
# summary(glmmod)
plot(glmmod, xvar = 'lambda', label = TRUE)
# plot(glmmod, xvar = 'dev', label = TRUE)
coef(glmmod)
plot(glmmod)

```
As lambda decreases, the order that covariates are added to the model:
    
    1. number_beds
    2. home_typeMunicipal
    3. short_stayYes
    4. X2y_withOrders_norm
    5. family_councilYes
    6. accreditationYes
    7. X2y_inspections_norm
    8. home_typeNon-Profit 
    

## LASSO - CV
```{r }
cv.glmmod <- cv.glmnet(x, y, alpha=1, family = 'binomial')
plot(cv.glmmod)
plot(cv.glmmod$glmnet.fit, label = TRUE, xvar = "lambda")
plot(cv.glmmod$glmnet.fit, label = TRUE, xvar = "dev")

coef(cv.glmmod)
coef_min <- tidy(coef(cv.glmmod, s = "lambda.min"))
coef_min

# write.csv(coef_min, file="../data_output/coef_lasso_min.csv")

```

```{r }
data_lr <- subset(df, select = c('outbreak',
                                 'number_beds',
                                 'home_type',
                                 'short_stay',
                                 'family_council',
                                 'residents_council',
                                 'accreditation',
                                 'X2y_withOrders_norm',
                                 'X2y_inspections_norm'
                                 ))


fit = glm(outbreak~., family='binomial', data=data_lr)
summary(fit)

# tidy_fit <- tidy(summary(fit)$coef)
# write.csv(tidy_fit, file="../data_output/logistic_regression_model.csv")

```

## Stepwise Model Selection
```{r, eval=FALSE, include=FALSE }
library(MASS)
# Start with model with no predictors - null
null <- glm(outbreak ~ 1, family = "binomial", data=data) # 1 here means the intercept 
full <- glm(outbreak ~ 
              home_type 
            + number_beds 
            + short_stay 
            + residents_council 
            + family_council 
            + accreditation 
            + total_inspections_norm 
            + X5y_inspections_norm 
            + X2y_inspections_norm 
            + total_withOrders_norm 
            + X5y_withOrders_norm 
            + X2y_withOrders_norm 
            , family = 'binomial', data = data)

# With AIC, k = 2 
modAIC <- stepAIC(full)
summary(modAIC)

# With BIC
modBIC <- stepAIC(full, k = log(nrow(data)))
summary(modBIC)

car::compareCoefs(modAIC, modBIC)
confint(modBIC)

stepAIC(full, scope=list(lower=null, upper=full), data=data, direction='backward', trace = 0)
stepAIC(null, scope=list(lower=null, upper=full), data=data, direction='forward', trace = 0)
stepAIC(null, scope=list(lower=null, upper=full), data=data, direction='both', trace = 0)
```

## Transforming the Predictors
```{r, eval=FALSE, include=FALSE}
# Assessing skewness
# Check for R skewness in numeric predictors
library(e1071)

listofcols = c('number_beds', complaints, noncomplaints, all_inspections)

for (each in listofcols){
  print(each)
  print(skewness(data[[each]]))
  qqnorm(data[[each]], main = each)
  qqline(data[[each]], col = 'red')
}

for (each in listofcols){
  print(each)
  print(summary(df[[each]]))
}

# Create a list of numeric variables with 0 values
list0s <- c(complaints, "X2y_noncomplaints_norm")

library(rcompanion)
listofcols <- setdiff(listofcols, list0s)

# Visualize the effect of square root and log transformations on the data without 0s
for (each in listofcols) {  
  plotNormalHistogram(x = data[[each]], main = each)
  plotNormalHistogram(x = sqrt(data[[each]]), main = c(each, 'sqrt trans'))
  plotNormalHistogram(x = log(data[[each]]), main = c(each, 'log trans'))
}

# Visualize the effect of a square root transformation on the variables with 0s
for (each in list0s) {  
  plotNormalHistogram(x = data[[each]], main = each)
  plotNormalHistogram(x = sqrt(data[[each]]), main = c(each, 'sqrt trans'))
}

# Log transform in place

data[c("total_noncomplaints_norm", "X5y_noncomplaints_norm", all_inspections)] <- log(data[c("total_noncomplaints_norm", "X5y_noncomplaints_norm", all_inspections)])

# Square root transform in place
data[c('number_beds', list0s)] <- sqrt(data[c('number_beds', list0s)])

skim(data)
```

```{r, eval=FALSE, include=FALSE }
# Calculate McFadden's pseudo R2
ll.null <- fit$null.deviance/-2
ll.proposed <- fit$deviance/-2
print((ll.null - ll.proposed)/ll.null)
1 - pchisq(2*(ll.proposed - ll.null), df = (length(fit$coefficients)-1)) #pvalue

```

## Visualize the model
```{r, eval=FALSE, include=FALSE}
# Create a new dataframe showing probability of outbreak and outbreak status
predicted.data <- data.frame(probability.of.outbreak=fit$fitted.values, outbreak=data$outbreak)

# Sort the above df
predicted.data <- predicted.data[order(predicted.data$probability.of.outbreak, decreasing=FALSE),]

# Add new col to df with rank
predicted.data$rank <- 1:nrow(predicted.data)

library(ggplot2)
# library(cowplot)

ggplot(data = predicted.data, aes(x=rank, y=probability.of.outbreak)) +
  geom_point(aes(color=outbreak), alpha = 0.5, shape = 4, stroke = 1) +
  xlab("Index") + ylab("Predicted probability of an outbreak") +
  ggtitle("Outbreak Status Ordered By Predicted Probability of an Outbreak") + 
  scale_color_manual(values = c('blue', 'red'))

```


```{r }

```


```{r }

```


```{r }

```



