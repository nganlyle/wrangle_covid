---
title: "BC LTC Analysis"
author: "KT Hobbs"
date: "02/08/2020"
output:
  html_document: default
  pdf_document: default
---

## Import and prepare the data
```{r results="hide"}
library(skimr)

# Import the data
df <- read.csv("../data/BC/bc_ltc_complete_kt.csv", na.strings="", header=TRUE)
colnames(df)
```
Remove homes with suppressed data (4) and special units (3) as they are included as part of a major building already.

```{r}
rmv <- c(# suppressed data
              "Bella Coola General Hospital", "Mackenzie and District Hospital and Health Centre",
                "Northern Haida Gwaii Hospital and Health Centre", "R. W. Large Memorial Hospital",
              
         # special care units
               "Berkley Care Centre - Special Unit", "Fair Haven - Vancouver - Special Unit",
              "Harmony Court Care Centre  - Special Unit")

df2 <- df[-which(df$FACILITY_NAME %in% rmv),]

# remove granular infractions
# df2 <- df2[, -grep('^INFRACTIONS_.', colnames(df2))]

# remove incidents that are NOT per 100 beds
colnames(df2)
df2 <- df2[, -c(22:29, 30:37)]
```


```{r}
# drop unneccessary columns
dr <- c(#"FACILITY_NAME", - keep for data quality analysis
        "HCC_CODE", "STREET_ADDRESS", "CITY", "POSTAL", # deemed irrelevant to inquiry
        "COMPLAINTS", # substantiated complaints are more telling
       "DCH_NURSE_LASTYR", "DCH_ALLIED_LASTYR", "DCH_TOTAL_LASTYR",
       "DCH_NURSE_CURRENTYR", "DCH_ALLIED_CURRENTYR", "DCH_TOTAL_CURRENTYR", 
       "THERAPY_PT", "THERAPY_RT", "THERAPY_OT", "RESTRAINTS",
       "ISE", # high multicollinearity
       "latitude", "longitude",
       # "AGE", # redundant - from pairs plot
       "Total.Confirmed.Cases", "Total.Deaths" # focus on outbreak status first
       )

df2 <- df2[, -which(names(df2) %in% dr)]

```

```{r}
# dataframe without health authority - 
df3 <- df2[, c(1, 3:length(df2))]
names(df3)
```


### Cleaning
  * Remove unneccesary `INFRACTIONS_`
  * Convert `FEMALE` and `AGE` variables to numeric (from factor) decimals
  * Assess collinearity
  * Convert NAs to "unknown"
  * Bin `FEMALE` and `AGE` variables
  

```{r}
df3 <- df3[,-c(26:37)]
```

  
```{r}
df3$FEMALE <- as.numeric(gsub("[\\%,]", "", df3$FEMALE))/100
df3$AGE <- as.numeric(gsub("[\\%,]", "", df3$AGE))
df3$AGE_85_PLUS <- as.numeric(gsub("[\\%,]", "", df3$AGE_85_PLUS))/100
df3$AGE_UNDER_65 <- as.numeric(gsub("[\\%,]", "", df3$AGE_UNDER_65))/100
```

```{r}
# assess age
pairs(df3[,c(10,12:13)])
```

"pairwise.complete.obs" then the correlation or covariance between each pair of variables is computed using all complete pairs of observations on those variables.

```{r}
cor(df3[,c(10,12,13)], use = "pairwise.complete.obs")
```


```{r}
# assess councils
# names(df3)
cor(df3[,c(28:29)])
```

I'm decided to keep `AGE_85_PLUS` because there are no missing variables (`AGE` has 3) and seems to be a comorbidity for COVID-19.


```{r}
df3 <- df3[,-c(10,13)]
```


```{r suppressMessages = TRUE}
library(tidyr)
library(arules)
# FEMALE
levels(discretize(df3$FEMALE, method = "interval", breaks = 3))

# convert NAs
df3$FEMALE <- replace_na(df3$FEMALE, "unknown")

df3[df3$FEMALE < 0.45,]$FEMALE <- "<0.45"
df3[df3$FEMALE > 0.45 & df3$FEMALE < 0.7,]$FEMALE <- "0.45-0.7"
df3[df3$FEMALE > 0.7 & df3$FEMALE < 0.95,]$FEMALE <- "0.7-0.95"



# AGE_85_PLUS
levels(discretize(df3$AGE_85_PLUS, method = "interval", breaks = 3))

# convert nas
df3$AGE_85_PLUS <- replace_na(df3$AGE_85_PLUS, "unknown")

df3[df3$AGE_85_PLUS < 0.34,]$AGE_85_PLUS <- "<0.34"
df3[df3$AGE_85_PLUS < 0.67 & df3$AGE_85_PLUS >= 0.34,]$AGE_85_PLUS <- "0.34-0.67"
df3[df3$AGE_85_PLUS <= 1 & df3$AGE_85_PLUS >= 0.67,]$AGE_85_PLUS <- "0.67-1"

```


  * Convert resident profile information to numeric
  * Convert`STAY_LENGTH`, `SUB_COMPLAINTS`, `CMI` to numeric
  
```{r}
# profile <- c("DEPRESSION", "ADL_DEPENDENT", "CPS_SEVERE", "DEMENTIA", "ABS_PHYS_ABUSIVE", 
            # "ISE_LOW", "MEDS_DEPRESSION", "MEDS_ANTIPSYCHOTICS")


df3$DEPRESSION <- as.numeric(gsub("[\\%,]", "", df3$DEPRESSION))/100
df3$ADL_DEPENDENT <- as.numeric(gsub("[\\%,]", "", df3$ADL_DEPENDENT))/100
df3$CPS_SEVERE <- as.numeric(gsub("[\\%,]", "", df3$CPS_SEVERE))/100
df3$DEMENTIA <- as.numeric(gsub("[\\%,]", "", df3$DEMENTIA))/100
df3$ABS_PHYS_ABUSIVE <- as.numeric(gsub("[\\%,]", "", df3$ABS_PHYS_ABUSIVE))/100
df3$ISE_LOW <- as.numeric(gsub("[\\%,]", "", df3$ISE_LOW))/100
df3$MEDS_DEPRESSION <- as.numeric(gsub("[\\%,]", "", df3$MEDS_DEPRESSION))/100
df3$MEDS_ANTIPSYCHOTICS <- as.numeric(gsub("[\\%,]", "", df3$MEDS_ANTIPSYCHOTICS))/100
```


```{r}
df3$STAY_LENGTH <- as.numeric(gsub("[,]", "", as.character(df3$STAY_LENGTH)))
df3$SUB_COMPLAINTS <- as.numeric(as.character(df3$SUB_COMPLAINTS))
df3$CMI <- as.numeric(as.character(df3$CMI))
```

### Assess NAs

```{r}
colSums(is.na.data.frame(df3))
``````



### Skewness and Transformations

```{r}
library(skimr)
```

`ROOMS_PRIVATE` distribution not improved with square transform.

`BEDS_TOTAL` distribution not imporved with log transform.

```{r}
# Create a list of right skewed numeric variables with 0 values
# colSums(df3 == 0)

list_non0s = c('BEDS_TOTAL')
list0s = c("ROOMS_PRIVATE", "ROOMS_SEMI", "ROOMS_MULTI",
  "BEDS_PRIVATEprop")


library(rcompanion)

# Visualize the effect of square root transformations on the numeric predictors
for (each in list0s) {  
  plotNormalHistogram(x = df3[[each]], main = each)
  plotNormalHistogram(x = sqrt(df3[[each]]), main = c(each, 'sqrt trans'))
}

# Visualize the effect of log transformations on the data with no 0s that are right skewed
for (each in list_non0s) {  
  plotNormalHistogram(x = df3[[each]], main = each)
  plotNormalHistogram(x = log(df3[[each]]), main = c(each, 'log trans'))
}


```

Apply transformations where suitable.

```{r}
transf = c("ROOMS_SEMI", "ROOMS_MULTI",
  "BEDS_PRIVATEprop")

df3[transf] <- sqrt(df3[transf])

skim(df3)
```

# Binary Logistic Regression

Remove `FACILITY_NAME`

```{r}
df4 <- df3[,-1]
names(df4)
```

NAs will be omitted by default.

**To include `AGE` or not?**

```{r}
fit2 = glm(outbreak~., family=binomial, data=df4, maxit = 25)
summary(fit2)
```


## LASSO 
**Should account for multicollinearity.**

```{r}
library(glmnet)
df_noNA <- na.omit(df4)

x <- model.matrix(outbreak~., df_noNA)[,-37]
y <- df_noNA$outbreak
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
plot(cv.lasso)
```

The plot displays the cross-validation error according to the log of lambda. The left dashed vertical line indicates that the log of the optimal value of lambda is approximately -5, which is the one that minimizes the prediction error. This lambda value will give the most accurate model. The exact value of lambda can be viewed as follow:

```{r}
coef(cv.lasso, cv.lasso$lambda.min)
```

Generally, the purpose of regularization is to balance accuracy and simplicity. This means, a model with the smallest number of predictors that also gives a good accuracy. To this end, the function cv.glmnet() finds also the value of lambda that gives the simplest model but also lies within one standard error of the optimal value of lambda. This value is called lambda.1se

```{r}
coef(cv.lasso, cv.lasso$lambda.1se)
```

Final LASSO model using `lambda.min`

```{r}
lasso.model <- glmnet(x, y, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.min)
lasso.model
```


