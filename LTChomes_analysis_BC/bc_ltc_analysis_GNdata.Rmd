---
title: "BC LTC Analysis"
author: "KT Hobbs"
date: "02/08/2020"
output:
  html_document: default
  pdf_document: default
---

## Import and prepare the data
```{r results="hide"}
library(skimr)

# Import the data
df <- read.csv("../data/BC/ngan_bcLTC_foranalysis.csv", na.strings="", header=TRUE)
colnames(df)
```


```{r}
# drop unneccessary columns
# incidents by category are removed, incidents per 100 beds is kept
rmv <- c("FACILITY_NAME", "cleaned_name", "facility_name", # removing number of dead because we are using outbreak as a response
  ) 

df2 <- df[, -which(names(df) %in% rmv)]

skim(df2)
```

Numerical Variables are all right-skewed. Since they all have similar distributions - is it alright to leave them as is (no transformation)?


#### Cleaning
```{r}
# Change time-related and ordinal categorical variables to factor
# yrs <- c('years_operating', 'AGE', 'AGE_85_PLUS', 'AGE_UNDER_65')
# 
# for (i in yrs){
#   df2[,i] <- as.factor(as.character(df2[,i]))
# }

# Convert beds, rooms, and infractions to a proportion of total
# BEDS
df2$BEDS_PRIV_PROP <- df2$BEDS_PRIVATE/df2$BEDS_TOTAL
df2$BEDS_PUB_PROP <- df2$BEDS_PUBLIC/df2$BEDS_TOTAL

# ROOMS
df2$ROOMS_TOTAL <- df2$ROOMS_PRIVATE + df2$ROOMS_SEMI + df2$ROOMS_MULTI
df2$ROOMS_PRIV_PROP <- df2$ROOMS_PRIVATE/df2$ROOMS_TOTAL
df2$ROOMS_SEMI_PROP <- df2$ROOMS_SEMI/df2$ROOMS_TOTAL
df2$ROOMS_MULTI_PROP <- df2$ROOMS_MULTI/df2$ROOMS_TOTAL


# Remove redundant cols
# removing AGE because AGE_UNDER_65 and AGE_85_AND_OVER are more descriptive
# remove complaints because they include unsubstantiated complaints. Same logic for inspections (infractions are more telling).

rmv2 <- c('BEDS_PRIVATE', 'BEDS_PUBLIC', 
          'ROOMS_PRIVATE', 'ROOMS_SEMI', 'ROOMS_MULTI',
          'AGE', 'COMPLAINTS', 'INSPECTIONS')

df3 <- df2[, -which(names(df2) %in% rmv2)]
```

Group data for ease later

```{r}
size <- c('BEDS_PRIV_PROP', 'BEDS_PUB_PROP', 'BEDS_TOTAL', 'ROOMS_PRIV_PROP', 'ROOMS_PUB_PROP', 'ROOMS_MULTI_PROP',
          'ROOMS_TOTAL')

resident_profile <- df3[,11:28]

quality <- df3[,c(2:10, 29:41)]

general<- df3[,42:48]

# colnames(df3)
```

Converting `INCIDENTS_*` and remaining resident profile predictors to numeric...

**NOTE:** 'suppressed' converted to NA by coercion

```{r, suppressMessages = T}
# resident profile params
df3[,names(resident_profile)] <- lapply(df3[,names(resident_profile)],FUN = as.character)
df3[,names(resident_profile)] <- lapply(df3[,names(resident_profile)],FUN = as.numeric)

# incidents
df3[,grep('^INCIDENT.',colnames(df3))] <- lapply(df3[,grep('^INCIDENT.',colnames(df3))], FUN = as.character)
df3[,grep('^INCIDENT.',colnames(df3))] <- lapply(df3[,grep('^INCIDENT.',colnames(df3))], FUN = as.numeric)

# sub complaints
df3$SUB_COMPLAINTS <- as.numeric(df3$SUB_COMPLAINTS)

# years operating
df3$years_operating <- as.numeric(as.character(df3$years_operating))

skim(df3)
```

**Assessing sparsity of dataframe and handling NAs:**


```{r handling NAs}
colSums(is.na(df3))
rowSums(is.na(df3))
```

NAs are often concentrated for particular homes and in incident/infraction predictors.

```{r col sparsity}
# columns with proportion of NAs and 0s > 0.01
colnames(df3)[colSums(is.na(df3) | df3 == 0)/(dim(df3)[1]*dim(df3)[2]) > 0.01]
```
```{r row sparsity}
# rows with proportion of NAs and 0s > 0.003 (those with almost all missing info)
# indices = rownames(df3)[rowSums(is.na(df3) | df3 == 0)/(dim(df3)[1]*dim(df3)[2]) > 0.003]
# df[indices,]$facility_name
```

**Remove sparse variables**
```{r}
sparse_cols <- colnames(df3)[colSums(is.na(df3) | df3 == 0)/(dim(df3)[1]*dim(df3)[2]) > 0.01]
# sparse_rows <- rownames(df3)[rowSums(is.na(df3) | df3 == 0)/(dim(df3)[1]*dim(df3)[2]) > 0.003]
df4 <- df3

df4 <- df4[, -which(names(df4) %in% sparse_cols)]
# df4 <- df4[-which(rownames(df4) %in% sparse_rows),]
colnames(df4)

dim(df4)
```


#### Transform Numerical Variables
```{r}
skim(df4)
```

No change in infractions distribution with sqrt or log transforms.

```{r}
# log transform
# infractions
df5 <- df4
df5$ROOMS_SEMI_PROP <- log(df4$ROOMS_SEMI_PROP)


par(mfrow=c(2,2))
hist(xtabs(~ROOMS_SEMI_PROP + BEDS_TOTAL, data=df4), main = "ROOMS_SEMI_PROP Untransformed")
hist(xtabs(~ROOMS_SEMI_PROP + BEDS_TOTAL, data=df5), main = "ROOMS_SEMI_PROP LOG Transformed")

# sqrt transform
hist(xtabs(~sqrt(ROOMS_SEMI_PROP) + BEDS_TOTAL, data=df4), main = "ROOMS_SEMI_PROP SQRT Transformed")



# log transform
# age under 65
df5 <- df4
df5$AGE_UNDER_65 <- log(df4$AGE_UNDER_65)


par(mfrow=c(2,2))
hist(xtabs(~AGE_UNDER_65 + BEDS_TOTAL, data=df4), main = "AGE_UNDER_65 Untransformed")
hist(xtabs(~AGE_UNDER_65 + BEDS_TOTAL, data=df5), main = "AGE_UNDER_65 LOG Transformed")

# sqrt transform
hist(xtabs(~sqrt(AGE_UNDER_65) + BEDS_TOTAL, data=df4), main = "AGE_UNDER_65 SQRT Transformed")

```

No change in infractions or incident distribution with log or sqrt transforms.

Original, skewed data will be used for now.


---

#### Categorical Assessment
```{r categorical assess}
# Inspect the distribution of the categorical variables

# hist(xtabs(~outbreak_status + FACILITY_NAME, data=df))
summary(df4$outbreak_status)

# hist(xtabs(~residents_council + FACILITY_NAME, data=df))
summary(df4$residents_council)
summary(df4$family_council)
summary(df4$home_type)

```


```{r, include = F}
# forward selection to determine cause of non-convergence
# df4 <- df4[, -grep('^INFRACTIONS_.', colnames(df3))]
# df4 <- df4[, -grep('INCIDENT.', colnames(df4))]
# df4 <- df4[,-c(7:20)] # health of residents
# df4 <- df4[,-c(3:5)] # age and gender proportions
# df4 <- df4[, -grep('._PROP$', colnames(df4))] # size props
# df4 <- df4[,-c(1:4, 12)] # remaining numericals, beds_total, sub_complaints, stay_length, infractions rooms_total


# just removing line 104 because it caused non-convergence
# trouble <- c('BEDS_TOTAL', "SUB_COMPLAINTS", "STAY_LENGTH", "INFRACTIONS", "ROOMS_TOTAL")
# df4 <- df4[,-which(names(df4) %in% trouble)]

# install.packages('')
# fit = glm(outbreak_status~., family=binomial('logit'), data=df4, method="detect_separation")
# summary(fit)

```

---

**Remove: `ROOMS_TOTAL` and `ISE` to eliminate multicolinearity according to VIF output.**

```{r}
df4 <- df4[,-which(names(df4) %in% c('ROOMS_TOTAL', 'ISE'))]
# colnames(df4)
```

## Binary Logistic Regression with NAs

```{r BLR with nas, eval = F}
fit2 = glm(outbreak_status~., family=binomial, data=df4, maxit = 25, na.action = na.pass)
summary(fit2)
```
`Error in glm.fit(x = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, : NA/NaN/Inf in 'x'`



## Binary Logistic Regression without cols that have NAs

```{r}
no_nas <- colnames(df4)[colSums(is.na(df4)) == 0]
df_no_null <- df4[names(df4) %in% no_nas]
```

```{r BLR without na cols}
fit3 = glm(outbreak_status~., family=binomial, data=df_no_null, maxit = 25)
summary(fit3)
```

## Binary Logistic Regression where NA ==> 0

Because each row represents an LTC, we do not want to omit any on the basis of NAs. Keeping NAs renders an error in LASSO so instead, 0s are imputed. ** Will this introduce bias? We dont know if LTCs that have NAs actually have high numbers or not.

```{r NAs}
# impute 0 where NAs exist
df4[is.na(df4)] <- 0

```


```{r BLR without NAs}
blr_fit <- glm(outbreak_status~., data = df4, family = binomial)
summary(blr_fit)
```


--- 

## Binary Logistic Regression Assumptions

assumption | check | result
----------|--------------|-------------
Linearity - linear relationship between continuous predictor variables and the logit of the outcome | visually - scatterplot | ok
Influential values | Cook's distance plot of model fit | no obvious influential points
Multicolinearity | `car::vif(model)` | **ISSUE 1:** potentially perfect multicollinearity between `ownership_type` and `home_type`. **ISSUE 2:** Variable inflation factor for assessing multicollinearity - `BEDS_TOTAL`, `ROOMS_TOTAL`, `ISE`, `ISE_LOW`  ~ $\ge 5$. **RESOLVED**

```{r linearity check, suppressMessages = TRUE, eval = F}
library(tidyverse)
library(broom)

mydata <- df4 %>%
  dplyr::select_if(is.numeric)
predictors <- colnames(mydata)

# Bind the logit and tidying the data for plot
probabilities <- predict(blr_fit, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "yes", "no")

mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)


ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") +
  theme_bw() +
  facet_wrap(~predictors, scales = "free_y")
```

```{r influential check, eval = F}
plot(blr_fit, which = 4, id.n = 3)
```

```{r multicol check, eval = F}
car::vif(blr_fit)
```


```{r, eval = F}
# Further assessing correlation
cor(df4$ISE, df4$ISE_LOW)

# cor(df4$ROOMS_TOTAL, df4$BEDS_TOTAL)
```

Warning in glm model implies potential separation of variables - for a particular value of a predictor, one outcome will always occur.
**Determined `ownership_typepublic` caused perfect separation**

---

## LASSO with Imputed 0s
LASSO removes multi-colinear predictors. 

```{r lasso, suppressMessages = TRUE}
library(glmnet)

# setup
# options(na.action="na.pass") #includes NA values

x <- model.matrix(outbreak_status~., data=df4)[,-42]
y <- df4$outbreak_status
grid=10^seq(10,-2,length=100)

fit_lasso = cv.glmnet(x,y, family="binomial", alpha = 1, lambda = grid, data=df4)
plot(fit_lasso)

```

Between 2-5 predictors are necessary.

```{r lassomin}
fit_lasso$lambda.min
```

Lambda defines the amount of shrinkage required to minimize prediction error.


```{r lasso coef}
coef(fit_lasso, fit_lasso$lambda.min)
```


##### LASSO on non-null cols

```{r lasso no nulls, suppressMessages = TRUE}
x <- model.matrix(outbreak_status~., data=df_no_null)[,-2]
y <- df_no_null$outbreak_status
grid=10^seq(10,-2,length=100)

fit_lasso2 = cv.glmnet(x,y, family="binomial", alpha = 1, lambda = grid, data=df4)
plot(fit_lasso2)

```

0 predictors are necessary.

```{r lassomin2}
fit_lasso2$lambda.min
```

Lambda defines the amount of shrinkage required to minimize prediction error.


```{r lasso2 coef}
coef(fit_lasso2, fit_lasso2$lambda.min)
```
---

### Factor Analysis on Mixed Data

```{r FAMD}
library("FactoMineR")
library("factoextra")

res <- FAMD(df4, graph = F)
eig.val <- get_eigenvalue(res)
head(eig.val)

fviz_famd_var(res, repel = TRUE)
```

quantitative variables
```{r}
fviz_famd_var(res, "quanti.var", repel = TRUE,
              col.var = "black", cex = 0.02)
```



